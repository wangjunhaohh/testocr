import cv2
import numpy as np
from pathlib import Path
from Levenshtein import distance as levenshtein_distance
import os


class OCRDictionaryCorrector:
    def __init__(self, word_list, err_corrections_dic):
        """
        åŸºäºè¯å…¸çš„OCRç»“æœæ ¡æ­£å™¨ï¼ˆå¢å¼ºå®¹é”™æœºåˆ¶ï¼‰
        :param word_list: é¢„å®šä¹‰è¯å…¸åˆ—è¡¨
        """
        self.word_set = set(word_list)
        self.usual_dictionary = err_corrections_dic

        # æ„å»ºå‰ç¼€æ ‘åŠ é€ŸåŒ¹é…
        self.trie = {}
        for word in word_list:
            node = self.trie
            for char in word:
                node = node.setdefault(char, {})
            node['#'] = True
    
    def correct(self, ocr_text, max_edit_distance=2, user_confirmation_threshold=2):
        """
        æ ¡æ­£OCRè¯†åˆ«æ–‡æœ¬ï¼ˆå¢å¼ºå®¹é”™èƒ½åŠ›ï¼‰
        :param ocr_text: OCRåŸå§‹è¯†åˆ«æ–‡æœ¬
        :param max_edit_distance: æœ€å¤§å…è®¸ç¼–è¾‘è·ç¦»
        :return: æ ¡æ­£åçš„æ–‡æœ¬
        """
        # 1. ç²¾ç¡®åŒ¹é…
        if ocr_text in self.word_set:
            return ocr_text, False
        
        if ocr_text in self.usual_dictionary.keys():
            return self.usual_dictionary[ocr_text], False
        
        # 2. å‰ç¼€æ ‘åŒ¹é…ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
        node = self.trie
        matched = ""
        for char in ocr_text:
            if char in node:
                matched += char
                node = node[char]
            else:
                break
        if '#' in node and len(matched) > 1:
            return matched, False
        
        # 3. ç¼–è¾‘è·ç¦»æ¨¡ç³ŠåŒ¹é…
        candidates = []
        for word in self.word_set:
            if abs(len(word) - len(ocr_text)) > max_edit_distance:
                continue
            ed = levenshtein_distance(ocr_text, word)
            if ed <= max_edit_distance:
                candidates.append((word, ed))
        
        if candidates:
            # è¿”å›ç¼–è¾‘è·ç¦»æœ€å°çš„è¯
            best_word, min_ed = min(candidates, key=lambda x: x[1])
            if min_ed <= user_confirmation_threshold:
                return best_word, False
            # ç¼–è¾‘è·ç¦»è¶…è¿‡é˜ˆå€¼éœ€è¦ç”¨æˆ·ç¡®è®¤
            return (ocr_text, best_word), True
        
        # 4. é•¿åº¦ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå®¹é”™æœºåˆ¶ï¼‰
        if not candidates:
            # é€‰æ‹©è¯å…¸ä¸­é•¿åº¦æœ€æ¥è¿‘çš„è¯
            len_diff = {w: abs(len(w)-len(ocr_text)) for w in self.word_set}
            min_diff = min(len_diff.values())
            close_words = [w for w, d in len_diff.items() if d == min_diff]        
            # åœ¨é•¿åº¦æ¥è¿‘çš„è¯ä¸­é€‰æ‹©ç¼–è¾‘è·ç¦»æœ€å°çš„
            if close_words:
                best_word = min(close_words, key=lambda w: levenshtein_distance(ocr_text, w))
                dist = levenshtein_distance(ocr_text, best_word)
                if dist <= user_confirmation_threshold:
                    return best_word, False
                # ç¼–è¾‘è·ç¦»è¶…è¿‡é˜ˆå€¼éœ€è¦ç”¨æˆ·ç¡®è®¤
                return (ocr_text, best_word), True
        
        # æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…é¡¹
        return ocr_text, False

def preprocess_image(image):
    """
    ä¼˜åŒ–ç‰ˆå›¾åƒé¢„å¤„ç†ï¼ˆé€‚åˆå°å°ºå¯¸æ–‡æœ¬ï¼‰
    åŒ…å«å›¾åƒä¿å­˜ç”¨äºè°ƒè¯•
    """
    # åŸå§‹å›¾åƒå¤‡ä»½
    original = image.copy()
    
    # 1. è½¬æ¢ä¸ºç°åº¦å›¾
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # 2. å¯¹æ¯”åº¦å¢å¼ºï¼ˆå¼±åŒ–å¤„ç†å¼ºåº¦ï¼‰
    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(4, 4))
    enhanced = clahe.apply(gray)
    
    # 3. Otsuå…¨å±€é˜ˆå€¼äºŒå€¼åŒ–
    _, binary = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    # 4. è½¬æ¢ä¸ºRGBä¸‰é€šé“
    processed = cv2.cvtColor(binary, cv2.COLOR_GRAY2RGB)
    
    return processed

def recognize_chinese_text(
    image,  # ä¿®æ”¹ï¼šç›´æ¥ä¼ å…¥PIL Imageå¯¹è±¡
    dictionary=None, 
    reader=None,
    err_corrections_dic=None
):
    """
    å¢å¼ºç‰ˆOCRè¯†åˆ«ï¼ˆä¼˜åŒ–å°å°ºå¯¸æ–‡æœ¬å¤„ç†ï¼‰
    :param image: PIL Imageå¯¹è±¡
    :param dictionary: æ ¡æ­£è¯å…¸
    :param use_gpu: æ˜¯å¦ä½¿ç”¨GPU
    :param model_dir: æ¨¡å‹å­˜å‚¨ç›®å½•
    :return: è¯†åˆ«ç»“æœåˆ—è¡¨[(æ–‡æœ¬, ç½®ä¿¡åº¦)]
    """

    # åˆå§‹åŒ–æ ¡æ­£å™¨
    corrector = OCRDictionaryCorrector(dictionary, err_corrections_dic)

    # ä¿®æ”¹ï¼šè½¬æ¢PIL Imageä¸ºOpenCVæ ¼å¼
    img = np.array(image)
    if img.size == 0:
        raise ValueError("ä¼ å…¥çš„å›¾åƒæ•°æ®ä¸ºç©º")
    
    # ä¿®æ”¹ï¼šé¢œè‰²ç©ºé—´è½¬æ¢ (RGB->BGR)
    if img.ndim == 3 and img.shape[2] == 3:
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    elif img.ndim == 3 and img.shape[2] == 4:
        img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)
    
    # è·å–å›¾åƒä¿¡æ¯
    h, w = img.shape[:2]
    c = 3 if img.ndim == 3 else 1
    
    # å›¾åƒé¢„å¤„ç†ï¼ˆåŒ…å«è°ƒè¯•è¾“å‡ºï¼‰
    processed_img = preprocess_image(img)
    
    # ä¼˜åŒ–è¯†åˆ«å‚æ•°ï¼ˆé’ˆå¯¹å°å°ºå¯¸æ–‡æœ¬ï¼‰
    results = reader.readtext(
        processed_img,
        detail=1,
        paragraph=False,
        text_threshold=0.2,        # é™ä½æ–‡æœ¬é˜ˆå€¼
        contrast_ths=0.05,         # è¿›ä¸€æ­¥æ”¾å®½å¯¹æ¯”åº¦é˜ˆå€¼
        width_ths=0.3,             # æ”¶ç´§æ–‡æœ¬æ¡†åˆå¹¶
        batch_size=1,              # å°å›¾ä½¿ç”¨å•æ‰¹æ¬¡
        decoder='beamsearch',      # æ›´ç²¾å‡†çš„è§£ç å™¨
        min_size=5,                # æœ€å°æ–‡æœ¬å°ºå¯¸
        rotation_info=None         # ç¦ç”¨æ—‹è½¬æ£€æµ‹ï¼ˆå°å›¾ä¸éœ€è¦ï¼‰
    )
    high_conf_results = []
    best_result = None
    max_prob = 0.0
    
    for i, item in enumerate(results):
        if len(item) >= 3:
            bbox, text, prob = item[:3]
            original_text = text.strip()
            
            # å§‹ç»ˆè®°å½•æœ€é«˜ç½®ä¿¡åº¦ç»“æœ
            if prob > max_prob:
                max_prob = prob
                best_result = (original_text, prob, bbox)
            
            # æ”¶é›†æ‰€æœ‰é«˜ç½®ä¿¡åº¦æ–‡æœ¬(>=0.2)
            if prob >= 0.2:
                high_conf_results.append({
                    'text': original_text,
                    'prob': prob,
                    'bbox': bbox
                })
    
    # æŒ‰ä»å·¦åˆ°å³æ’åºï¼ˆåŸºäºå·¦ä¸Šè§’Xåæ ‡ï¼‰
    if high_conf_results:
        high_conf_results.sort(key=lambda x: min([pt[0] for pt in x['bbox']]))
    # ç»“æœå¤„ç†ï¼ˆå¼ºåˆ¶ä¿ç•™æœ€é«˜ç½®ä¿¡åº¦ç»“æœï¼‰
    full_text = ''.join([item[1] for item in results if item[2] >= 0]).replace(' ', '').replace('|', 'ä¸¨')
    
    # æ•´ä½“å­—ç¬¦ä¸²æ ¡æ­£
    if dictionary and full_text:
        corrector = OCRDictionaryCorrector(dictionary, err_corrections_dic)
        corrected_text = corrector.correct(full_text)
        return corrected_text, max_prob
    
    return full_text, max_prob

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import loader
    # ç¡®ä¿è°ƒè¯•ç›®å½•å­˜åœ¨
    debug_dir = Path("debug")
    debug_dir.mkdir(exist_ok=True)
    
    # é¢†åŸŸè¯å…¸ç¤ºä¾‹
    domain_dict = loader.load_player_dic("data/testData.json")
    
    # æµ‹è¯•å›¾åƒè·¯å¾„
    image_path = "output/test_player_name.jpg"
    
    print("\n" + "="*50)
    print(f"ğŸ” æµ‹è¯•-å¼€å§‹è¯†åˆ«: {image_path}")
    print("="*50)
    
    # æ‰§è¡ŒOCRè¯†åˆ«
    results = recognize_chinese_text(
        image_path=image_path,
        dictionary=domain_dict,
        use_gpu=True
    )
    
    # ç»“æœç»Ÿè®¡
    print("\n" + "="*50)
    print("è¯†åˆ«ç»“æœç»Ÿè®¡:")
    print("="*50)
    print(f"{'æ–‡æœ¬':<20} | {'ç½®ä¿¡åº¦':<8} | {'è¯å…¸åŒ¹é…'}")
    print("-" * 45)
    for text, prob in results:
        match = "âœ“" if text in domain_dict else "âœ—"
        print(f"{text:<20} | {prob:.4f}   | {match}")
    
    # è¡¥å……è¯´æ˜
    if not results:
        print("\nâŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼Œè¯·æ£€æŸ¥:")
        print("- é¢„å¤„ç†å›¾åƒæ˜¯å¦åŒ…å«å¯è¯»æ–‡æœ¬ (debug/*_processed.jpg)")
        print("- åŸå§‹è¯†åˆ«è¯¦æƒ…ä¸­çš„ç½®ä¿¡åº¦å’Œæ–‡æœ¬å†…å®¹")
        print("- å°è¯•è°ƒæ•´ text_threshold å‚æ•°å€¼")